---
title: "Encounter Classifier"
author: "Yvonne Barkley"
date: "April 16, 2018"
output: html_document
---

```{r setup}
    knitr::opts_knit$set(root.dir = normalizePath("C:\\Users\\yvers\\Documents\\CHP 1\\code\\Chp1_PcClassifier")) 
```


```{r warning=FALSE, message=FALSE}
library(sqldf)
library(randomForest)
library(party)
library(caret)
library(corrplot)
library(parallel); 
library(doParallel);
library(gtools)
library(devtools) 
library(dplyr)
library(data.table)
library(ggplot2)

pcdata <- read.csv('data/BigRoccaFile_PMN_TowedAll_EDIT_20181124.csv')
pcdata$group[pcdata$EncounterID == 'M2'] = 32
pcdata$group[pcdata$EncounterID == 'M3'] = 33
pcdata$group[pcdata$EncounterID == 'M4'] = 34
pcdata$group[pcdata$EncounterID == 'M1'] = 31
pcdata$group[pcdata$EncounterID == 'N2'] = 15
pcdata$group[pcdata$EncounterID == 'N3'] = 16
pcdata$group[pcdata$EncounterID == 'N4'] = 17
pcdata$group[pcdata$EncounterID == 'N1'] = 14
pcdata$group[pcdata$EncounterID == 'P6'] = 6
pcdata$group[pcdata$EncounterID == 'P1'] = 1
pcdata$group[pcdata$EncounterID == 'P2'] = 2
pcdata$group[pcdata$EncounterID == 'P4'] = 4
pcdata$group[pcdata$EncounterID == 'P5'] = 5
pcdata$group[pcdata$EncounterID == 'P3'] = 3
pcdata$group[pcdata$EncounterID == 'P8'] = 8
pcdata$group[pcdata$EncounterID == 'P7'] = 7



pcdata.M <- droplevels(filter(pcdata, pcdata$population == 'MHI'))
pcdata.N <- droplevels(filter(pcdata, pcdata$population == 'NWHI'))
pcdata.P <- droplevels(filter(pcdata, pcdata$population == 'Pelagic'))

```




Test correlation of variables for each POPULATION separately 
Then run Random Forests for each POPULATION

##MHI correlation
```{r}    
#Using only continuous variables, determine the correlated variables within the MHI whistles
pcdata.M <- droplevels(filter(pcdata, pcdata$population == 'MHI'))
pcsub<-pcdata.M[,7:53]
#corrplot.mixed(cor(pcsub, method= "pearson"), upper="number", lower="circle")
tmp <- cor(pcsub)
tmp[upper.tri(tmp)] <- 0
diag(tmp) <- 0
pcsubcor_M <- pcsub[,!apply(tmp,2,function(x) any(x > 0.80 | x < -0.80))]

pcdata.Muncorr <- cbind(pcdata.M[, c(1:6)], pcsubcor_M)

pcdata.Muncorr <- pcdata.Muncorr[order(pcdata.Muncorr$EncounterID),]


```

##NWHI correlation
```{r}    
#Using only continuous variables, determine the correlated variables within the NWHI whistles
pcsub<-pcdata.N[,7:53]
#corrplot.mixed(cor(pcsub, method= "pearson"), upper="number", lower="circle")
tmp <- cor(pcsub)
tmp[upper.tri(tmp)] <- 0
diag(tmp) <- 0
pcsubcor_N <- pcsub[,!apply(tmp,2,function(x) any(x > 0.80 | x < -0.80))]

pcdata.Nuncorr <- cbind(pcdata.N[, c(1:6)], pcsubcor_N)
pcdata.Nuncorr <- pcdata.Nuncorr[order(pcdata.Nuncorr$EncounterID),]

#levels(pcdata.Nuncorr$EncounterID) <- c("N1", "N2", "N3", "N4" )

```
##Pelagic correlation

```{r}    
##Using only continuous variables, determine the correlated variables within the Pelagic whistles
pcsub<-pcdata.P[,7:53]
#corrplot.mixed(cor(pcsub, method= "pearson"), upper="number", lower="circle")
tmp <- cor(pcsub)
tmp[upper.tri(tmp)] <- 0
diag(tmp) <- 0
pcsubcor_P <- pcsub[,!apply(tmp,2,function(x) any(x > 0.80 | x < -0.80))]

pcdata.Puncorr <- cbind(pcdata.P[, c(1:6)], pcsubcor_P)

#levels(pcdata.Puncorr$EncounterID) <- c("P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8" )
pcdata.Puncorr <- pcdata.Puncorr[order(pcdata.Puncorr$EncounterID),]
```

```{r}
#pcdata <- read.csv('data/BigRoccaFile_PMN_TowedAll_EDIT_20181124.csv')


PEL = 1:8                # designated groupID's assigned for each pelagic group 
NWHI  = 14:17   # designated groupID's assigned for each NWHI group, #20 was absorbed into #19 (June 2017)     
MHI = 31:34    # designated groupID's assigned for each MHI group, #25 absorbed into #26 (June 2017) 
                              # Oct 7: more combining of NWHI and MHI occurred. Performed in Pc_DatManipulation.rmd
#This is set up to be flexible to change later if needed
#nrep  = 3          # 7/1/18, no need, running all groups 
npel  = 1           # number of pelagic schools to group together
nnwhi = 1           # number of northwest HI schools to group together
nmhi  = 1           # number of main HI schools to group together
w = 150              # Number of whistles randomly pulled from each group 
ntest = 1           # number of groups to randomly pull for test dataset
n_samps = 100       #Zack: number of times you want to resample from your dataset
prop = 0.75          #proportion of whistles in training data
```

##Random Forest
```{r}
####Start####
today = 20190326
POP = MHI
pop.data = pcdata.M


ptm <- proc.time()

results.enc = matrix(0, nrow = 4, ncol = 4) #based on total # of groups
realP = c()
fakeP = c()
realM = c()
fakeM = c()
output_WhistleClass.enc =NULL
output_EncounterClass.enc=NULL
output_RawResults.enc=NULL
output_TrainSet.encAll=NULL
output_TestSet.encAll = NULL
output_Votes.encAll=NULL
tuned.encAll=NULL
varIMPtotal.enc =NULL
## GIANT LOOP---BEWARE ####

falsehope=c()

for(rep in 1:n_samps){

#7/1/18 Since I'm only interested in how a population classifies to encounter separately, I will run separate RF for each population using encounter as the target variable
  
#7/20/18 This is a totally random RF using the same partitioning as the Encounter Classifier
  
#From all of the encounters, sampling prop whistles from each encounter
enc_all = NULL
for(i in c(POP)){
  enc_sub=droplevels(subset(pop.data, pop.data$group==i)) #selects all whistles from the randomly selected groups, drops empty levels too
  enc_samp=enc_sub[sample(nrow(enc_sub), w),] #randomly samples w whistles from total whistles
  enc_all <-rbind(enc_all, enc_samp)
}


#Correlation Step####
pcsub<-enc_all[,7:53]
tmp <- cor(pcsub)
tmp[upper.tri(tmp)] <- 0
diag(tmp) <- 0
pcsubcor <- pcsub[,!apply(tmp,2,function(x) any(x > 0.80 | x < -0.80))]
enc_all <- cbind(enc_all[, c(1:6)], pcsubcor)


enc_part <- createDataPartition(y=enc_all$EncounterID,p=prop,list=FALSE)
enc_train <- enc_all[enc_part,]  
enc_test <- enc_all[-enc_part,]


##############################
## Set up a dataframe where each row is a specific 
## combination of mtry (# of variables selected at each split), n_tree (# of trees in  forest), possibly other stuff
##############################
mtry_vals = sqrt(length(enc_all[,7:length(enc_all)])) 
ntree_vals = seq(501,5001,500) #from, to, by
tune_settings2 = expand.grid(mtry = mtry_vals, ntree = ntree_vals)

#################################
## Initiate Cluster
#################################
cl = makeCluster(detectCores())
registerDoParallel(cl, cores = detectCores())

############################################
##Create the formula for the RF model with 'population' as the response and time-frequency measurements from uncorrelated data as the covariates
############################################
#formPc.new <- as.formula(paste("population ~ ", paste(names(pcnew)[7:36],collapse="+")))
formPc_enc <- as.formula(paste("EncounterID ~ ", paste(names(enc_all)[7:length(enc_all)],collapse="+"))) #Use variables to classify to encounter

#######################################
##The foreach function is the loop function for parallel processing. 
##It uses %dopar% which partitions the loop among your cores, then combines each output using the .combine argument. 

x = foreach(i = 1:nrow(tune_settings2), .combine = rbind) %dopar% { 
  library(randomForest)
  do_RF2 = randomForest(formPc_enc, data = enc_train, test = enc_test, ntree = tune_settings2$ntree[i], mtry = tune_settings2$mtry[i])
  print(do_RF2$err.rate[tune_settings2$ntree[i],])
}

#so, for each of the parameter combinations, i.e., rows of the tune_settings df, the RF is run using those 
#settings (line 46) and the error rates for the 3 pop's and the oob are printed (line 47). 
#when all the rows are finished, the results are combined using the rbind function into one matrix, stored as x.

#NB: .combine can be anything, really, for example .combine = c for a vector, .combine = list for a list, etc.

#################################
## Stop Cluster
#################################
stopCluster(cl)

########################################
## Combine settings and results
########################################
tune_settings2 = cbind(tune_settings2, x)
########################################
## Optimal Settings: the settings combo
## that has the lowest OOB rate
#######################################
tuned.enc<-tune_settings2[which.min(tune_settings2$OOB),]
tuned.encAll <- rbind(tuned.encAll, tuned.enc)
 
### RANDOM FOREST ####
accuracy.enc <- c()
enc_pred.table=NULL


##Run Random Forest model using 'tuned' parameters for mtry and ntree
enc_fit <- randomForest(formPc_enc, data=enc_train, ntree=tuned.enc[1,2], importance=TRUE, replace=T, mtry=tuned.enc[1,1], norm.votes=T, confusion=T, keep.inbag=T, proximity=T) 

enc_prediction <-predict(enc_fit, newdata=enc_test, predict.all = F, type='response') #prediction for encounter of test data using RF model, 'fit'
enc_prediction_vote <-predict(enc_fit, newdata=enc_test, predict.all = T, type='vote') #prediction for encounter of test data using RF model, 'fit'
enc_test$rightPred <- as.character(enc_prediction) == as.character(enc_test$EncounterID) #shows T/F for individual whistles and whether they were classified correctly
accuracy.enc <- sum((enc_test$rightPred)/nrow(enc_test)) #sums all 'TRUE' and divides by total

###RESULTS!!!##################################  
enc_results.temp <- table(enc_test$EncounterID, enc_prediction)    
results.enc <- results.enc + enc_results.temp  


# Important Variables
  varIMPtemp.enc <- as.data.frame(enc_fit$importance)
  varIMP.enc <- varIMPtemp.enc[order(varIMPtemp.enc$MeanDecreaseAccuracy), , drop =F]
  varIMPtotal.enc <- cbind(rep, rbind(varIMPtotal.enc, varIMP.enc))


if( (rep %% 10) == 0){ # take rep, divide by 10, outputs remainder
  print(paste("Done with", rep, "step"))
} 

}

proc.time()-ptm
```


```{r}


fn = sprintf("_%s_%s.csv", unique(pop.data$population), today)

write.table(results.enc, file= paste('PcResults_Encounters_TotalResults', fn, sep=""), append = F, row.names = T, col.names=T, sep = ",")

write.table(varIMPtotal.enc, paste('PcResults_Encounters_ImpVar',fn,sep=""), sep=",", append=TRUE)


```
Kappa Calculations
```{r}
confmat <- read.csv('PcResults_Encounters_TotalResults_MHI_20190326.csv', header=T, row.names = 1)

#matrix dimensions
# m = 4
# n = 5

confmat2 = as.matrix(confmat)
total_obs = sum(confmat2[ , ])

#Observed Accuracy
obs_acc <- sum(diag(confmat2))/total_obs

#Expected Accuracy
exp_acctemp <- (colSums(confmat2)*rowSums(confmat2))/total_obs
exp_acc <- sum(exp_acctemp)/total_obs

#kappa coefficient
k = (obs_acc-exp_acc)/(1-exp_acc)

```


